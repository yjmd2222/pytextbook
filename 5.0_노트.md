# 전처리
- 소숫점: 고성능 전처리 라이브러리는 자동적으로 잘 처리하겠지만 기초부터 시작한다면 숫자를 제거하는 부분이 있다. 그런데 단순히 `re.sub('[0-9],...)`를 한다면 소숫점은 남을 수 있음. 그러면 소숫점의 숫자를 포함하는 문장 하나가 여러 문장으로 잘 못 전처리 될 수 있다.
    - 한 가지 해결방법: `re.sub('-?[0-9]+\.?[0-9]+?|-?[0-9]+',...)`
        - `-?`:  음수 처리
        - `[0-9]+`: float의 정수 부분 또는 두 자리 수 이상의 수 처리
        - `\.?`: 소숫점 처리
        - `[0-9]+?`: float의 decimal 부분 처리
        - `|-?[0-9]+`: 한 자리 수 처리.
        - `|`로 구분된 `pat`에서 앞의 것이 먼저 처리되고 뒤에 것이 나중에 처리된다.
            - float 먼저 처리하고 처리 안 된 한 자리 수 처리하게 된다.
            - 거꾸로 한다면 모든 숫자를 성수로 인식하고 처리한 다음 소수를 처리하고자 하는데, 숫자가 하나도 남지 않고 소숫점만 있기 때문에 원하는 결과가 나오지 않는다.

# Vectorizer
- 이 책에서는 주로 count-based vectorizer를 다루는 것 같다. 다양한 종류의 vectorizer가 있는데, 나중에 더 학습해보기.

# 불순도 Impurity
- 특정 카테고리(파티션)에서 datapoint를 골라서 이를 분포에 맞게 타겟 값을 랜덤으로 부여할 때 그것이 잘못 라벨링 되는 정도.
    - 90/10의 비율을 이루는 feature가 있을 때 90/10은 대부분이 Yes이기 때문에 잘 맞 출 것이고, 50/50의 비율을 이루는 경우는 절반만 잘 맞출 것이다.
    - 이 경우 90/10의 impurity가 낮다.
    - 과정
        - Yes와 No에 대해 각각 impurity를 구하고 weighted average를 구해서 이것이 낮은 feature를 선택해서 root에 배치
        - Yes와 No로 내려간 노드에서 반복
        - 가장 잘 분류 시도한 후 다음 조건으로 다시 가장 잘 분류 시도한다고 생각할 수 있다.
        - 여러 종류의 impurity가 있으며, 대표적으로 Gini impurity가 있음.
            - Gini Impurity for $i$-th feature: $\text{gi}_i = 1 - \Sigma_j p_{i,j}^2 = \Sigma_j p_{i,j} (1 - p_{i,j})$
    - impurity 말고 entropy를 극대화할 수도 있다.

# Gradient Boosting(6장)
- 모델이 잘 학습하지 못했던 부분을 집중적으로 다음 모델에 학습
    - 잔차(Residual): 실제값과 예측값과의 차이
    - 이 잔차를 학습시켜 잔차를 줄여나가도록 한다

# Cross-validation
- 왜: 여러 폴드로 학습 데이터를 나누어 평균 성능을 확인할 수 있다. 성능의 편차가 심하면 추가 튜닝이 필요하다는 것을 생각해볼 수 있음. 시각화를 돕기 위한 메소드다.
- `cross_val_predict()` 작동 원리: 학습 후 입력 데이터포인트 `X`가 들어왔을 때 이 `X`가 여러 폴드 중 하나의 폴드의 `X_test`의 데이터포인트에 해당하면 그때 `pred`를 준다.
    - 이 데이터포인트가 여러 폴드에 걸쳐 있거나 없으면 오류가 남. [Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised)](https://scikit-learn.org/stable/modules/cross_validation.html#obtaining-predictions-by-cross-validation)
- 그래서 이걸 결과 모델로 사용하지 않는다. 평균 성능을 확인한 후 전체 학습 데이터에 대해 다시 학습하는 것을 전제로 확인하는 것.