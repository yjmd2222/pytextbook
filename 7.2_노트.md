이제까지 다룬 방법은 코퍼스 내에 단어가 있을 때 그 횟수나 tf-idf를 계산한 것. 순서를 다루지 않는다.

순서를 다루는 방법으로 distributed representation로, 대표적으로 Word2Vec에서 다루는 CBoW와 Skip-gram이 있음. 하나의 단어 비워두고 예측하는 것과, 하나의 단어만 두고 나머지를 비워두고 예측하는 것.

주로 many-to-one로 예측할 듯. 감정분석, 토픽분석 등. 이 장에서 다루는 RNN은 many-to-one, many-to-many가 가능하다.

# oov
oov: out-of-vocabulary. 학습 완료한 토크나이저가 새롭게 확인하는 단어는 oov가 됨. 보통 `"<oov>"` 토큰으로 대체한다.
- vocab_size를 제공하는 tokenizer에서 이 size 제한을 넘는(빈도가 상대적으로 적은) 토큰들도 학습할 때 `"<oov>"`로 대체된다.

# batch normalization
- [1](https://cvml.tistory.com/5)
- [2](https://gaussian37.github.io/dl-concept-batchnorm/)
기울기 소실을 방지하기 위한 방법 중 하나. sigmoid/tanh에서 큰 값의 boundary를 정해버리는 것, 자동 가중치 초기화, 작은 lr로 학습하는 것 외에 사용하는 방법이다.

- covariate shift: 학습과 테스트 데이터 분포가 다르기 때문에 예측 잘 못 함
- internal covariate shift: 각 레이어마다 매번 들어오는 데이터 분포가 다르기 때문에 shift가 심화됨
- 그래서 매번 학습하는 배치들의 분포를 일정하게 맞춰주기
    - 꼭 학습 과정 안에 이 부분이 요소로 포함되어야 함. 순전/역전파랑 상관 없이 이루어지면 whitening 현상이 일어난다고 함.


# 실습 중 노트
- `LSTM`의 parameter `return_sequences=True`: 모든 시점에 대해 은닉 상태를 출력함.
    - `True`: many-to-many
    - `False`: many-to-one
    - 내부에 many-to-many(nXm)를 사용하고 맨 끝에 many-to-one(mx1)을 하면 결과적으로 얻는 것은 many-to-one(nx1)이다.